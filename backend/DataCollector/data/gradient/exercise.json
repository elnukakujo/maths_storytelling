[
    {
      "question": "What is the primary purpose of using Gradient Descent in optimization?",
      "options": [
        "To find the steepest uphill path to maximize a function",
        "To iteratively move towards the lowest point of a function",
        "To calculate the average value of a function",
        "To find the middle point of a function's range"
      ],
      "answer": "To iteratively move towards the lowest point of a function"
    },
    {
      "question": "In Gradient Descent, what does the gradient tell us?",
      "options": [
        "The function's average rate of change",
        "The direction of the steepest ascent",
        "The distance to the function's minimum point",
        "The function's highest possible value"
      ],
      "answer": "The direction of the steepest ascent"
    },
    {
      "question": "In training a model, imagine you’re adjusting its settings to improve its performance, like adjusting knobs to make a radio sound clearer. When you turn a knob and hear a clearer sound, you keep turning in that direction. How is this process similar to using gradients in optimizing a model?",
      "options": [
        "It’s different because turning knobs doesn’t provide feedback on the best direction",
        "It’s similar because each adjustment goes directly to the final setting",
        "It’s similar because each adjustment moves in the direction that improves performance the most",
        "It’s different because model optimization doesn’t involve gradual adjustments"
      ],
      "answer": "It’s similar because each adjustment moves in the direction that improves performance the most"
    },
    {
      "question": "What does it mean when the gradient is zero?",
      "options": [
        "The function has reached a maximum",
        "The function is at a flat point or minimum",
        "The function has been reset to the starting point",
        "The function’s average value has been reached"
      ],
      "answer": "The function is at a flat point or minimum"
    },
    {
      "question": "What is the purpose of taking small, iterative steps in Gradient Descent?",
      "options": [
        "To move as quickly as possible towards the function's peak",
        "To accurately follow the function's steepest incline",
        "To avoid missing the global minimum and ensure gradual convergence",
        "To calculate the average slope of the function"
      ],
      "answer": "To avoid missing the global minimum and ensure gradual convergence"
    }
  ]
  